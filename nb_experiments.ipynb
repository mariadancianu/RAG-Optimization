{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0accd0d-fb0b-4c5e-be02-8063d3b6a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariadancianu/Desktop/Git Projects/RAG-Optimization/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from rag_optimization import convert_knowledge_base_to_langchain_docs, optimize_rag_parameters\n",
    "from data_utils import convert_json_to_dataframe, create_json_subset, collect_all_results, merge_results\n",
    "sns.set_style(\"whitegrid\")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953814b1",
   "metadata": {},
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful material \n",
    "# SQuAD Evaluation guidelines: \n",
    "# https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/\n",
    "# https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b749b",
   "metadata": {},
   "source": [
    "# Convert json data to pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d8df1-cea0-410b-9c27-985406ea208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_json_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = pd.read_csv(\"dataset.csv\")\n",
    "df_all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771778e-ae9e-49b4-9216-658bdefc8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2eebd-70b9-4118-b691-0c6c96e72a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some rough statistics for the context length \n",
    "df_selected.loc[:, \"context_chars\"] = df_selected[\"context\"].apply(lambda x: len(x))\n",
    "df_selected.loc[:, \"context_words\"] = df_selected.loc[:, \"context\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f66f8e-3eb9-4943-9b2e-40df68068a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cab993-b3ab-4947-acb5-016382d1321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "sns.histplot(df_selected.drop_duplicates(subset=\"context\")[\"context_chars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb1095-e0d8-4f06-b203-08ff1689aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "sns.histplot(df_selected.drop_duplicates(subset=\"context\")[\"context_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9848fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create original json structure for only a subset of questions, used for tests and fine-tuning \n",
    "# this file will be used by the evaluation.py file \n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_sel = df[0:500]\n",
    "df_sel.head(2)\n",
    "\n",
    "create_json_subset(df_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c8057-750e-44d6-81c8-d8ad6717b311",
   "metadata": {},
   "source": [
    "# RAG architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf8664-3381-4701-b297-bbcfa5176872",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "**Data Indexing**\n",
    "\n",
    "Converting text data into a searchable database of vector embeddings, which represent the meaning of the text in a format that computers can easily understand.\n",
    "- **Documents Chunking**: The collection of documents is split into smaller chunks of text. This allows for more precise and relevant pieces of information to be fed into the language model when needed, avoiding information overload.\n",
    "- **Vector Embeddings**: The chunks of text are then transformed into vector embeddings. These embeddings encode the meaning of natural language text into numerical representations.\n",
    "- **Vector Database**: Finally, the vector embeddings are stored in a vector database, making them easily searchable.\n",
    "\n",
    "**Documents -> Text chunks -> Vector Embeddings -> Vector DB**\n",
    "\n",
    "**Load -> Split -> Embed -> Store**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfc599",
   "metadata": {},
   "source": [
    "## Convert the pandas context to Langchain documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f241f-d54c-452e-94ad-cfcc8a81a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8e92a-9322-4359-b18b-f4217fb78003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(langchain_docs))\n",
    "print(langchain_docs[0])\n",
    "print(langchain_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2075d54",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_optimization import CustomRAG, prompt_message, convert_knowledge_base_to_langchain_docs\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"chromadb\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-large\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "    \"llm\": {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"client\": \"OpenAI\"\n",
    "        }\n",
    "}\n",
    "\n",
    "df_to_test = df[0:10]\n",
    "\n",
    "rag = CustomRAG(knowledge_base=langchain_docs, \n",
    "                prompt_message=prompt_message,\n",
    "                config_dict=parameters_dict, \n",
    "                results_folder='./eval_results/test_new_class', \n",
    "                vector_db_folder='./vector_databases/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.vector_store.initialize_embeddings_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9878b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = \"text-embedding-3-small\"\n",
    "embeddings = OpenAIEmbeddings(model=embeddings_model)\n",
    "\n",
    "db_dir = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "rag.vector_store.create_vector_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0571139",
   "metadata": {},
   "source": [
    "## Querying the vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5fcd1-6677-4a48-823c-7b899bfee9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is the weather today in Milan?\"\n",
    "relevant_docs = rag.vector_store.query_vector_store(query, n_results=3, score_threshold=0.1)\n",
    "\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who were the normans?\"\n",
    "context = rag.vector_store.query_vector_store(query, n_results=3, score_threshold=0.1)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5aa673",
   "metadata": {},
   "source": [
    "## Run the RAG over a subset of questions and save the answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_test = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df_to_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff098d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag.get_llm_multiple_questions_answers(df_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204b4fa",
   "metadata": {},
   "source": [
    "## RAG Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273535f0",
   "metadata": {},
   "source": [
    "TEXT CHUNKING \n",
    "\n",
    "1. CHARACTER SPLITTING : divide the text into N-character sized chunks. Can split words in the middle. \n",
    "2. RECURSIVE CHARACTER SPLITTING: preserves sentences. Avoids splitting sentences midword (note that RecursiveCharacterTextSplitter with separator does exactly that). Split the\n",
    "document where a double new line is present, then, if the chunk size is still exceeded, split at new lines, and so on.\n",
    "3. SEMANTIC SPLITTING: keeps related content together. Use embeddings to split based on meaning.\n",
    "+ other techniques\n",
    "\n",
    "EMBEDDINGS \n",
    "Create fixed-length vector representation of text, focusing on semanting meaning for tasks like similarity comparison. \n",
    "Most up to date embedding models, both proprietary and open source, with performance metrics across different tasks: https://huggingface.co/spaces/mteb/leaderboard.\n",
    "\n",
    "This contains also a \"retrieval\" column with performance metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0830832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    results_folder=\"eval_results/test_new_class\", \n",
    "    vector_db_folder=\"vector_databases/test_new_class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"eval_results/optimize_results\"\n",
    "df_all_res = collect_all_results(path)\n",
    "df_all_res.sort_values(by=\"HasAns_f1\", ascending=False, inplace=True)\n",
    "df_all_res.to_csv(f\"{path}/df_all_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c923406",
   "metadata": {},
   "source": [
    "# Investigate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008528cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best results and merge the scores by question id to the original df in order to inspect the errors.\n",
    "# The idea is to understand why the results are so poor for the NoAns questions, when the HasAns questions have \n",
    "# a high f1 score, in order to understand how the workflow can be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeaa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_path = os.path.join(os.getcwd(), \"eval_results/initial_eval_results\", df_all_res.experiment.iloc[0])\n",
    "split_path = best_result_path.split(\"/\")\n",
    "split_path[-1] = split_path[-1].replace(\"eval_\", \"\")\n",
    "best_result_path = \"/\".join(split_path)\n",
    "best_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1beec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python \"$(pwd)/eval_results/evaluation.py\" \"$(pwd)/eval_results/data_updated_500.json\" \"$(pwd)/eval_results/optimize_results/pred_400_all-MiniLM-L6-v2_gpt-3.5-turbo.json\" --out-file \"$(pwd)/eval_results/optimize_results/eval_pred_400_all-MiniLM-L6-v2_gpt-3.5-turbo.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_results(f1_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/f1_thresh_by_qid.json\"), \n",
    "                          exact_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/exact_thresh_by_qid.json\"), \n",
    "                          pred_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"), \n",
    "                          filepath_500=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"),\n",
    "                          context_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/context_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"), \n",
    "                          df_questions_filepath=\"dataset.csv\", \n",
    "                          filter_500=True)\n",
    "\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585095c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9391a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.is_impossible][[\"id\", \"is_impossible\", \"f1_score\", \"exact_score\", \"question\", \"pred\"]].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.loc[479, \"context\"].replace(\". \", \".\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.loc[479, \"rag_retrieved_context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cef210",
   "metadata": {},
   "source": [
    "# Evaluate with other LLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with best parameters, and save also the context\n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "        \"text-embedding-3-small\": \"OpenAI\", \n",
    "        },\n",
    "    \"models\": {\n",
    "       # \"meta/meta-llama-3-70b-instruct\":\n",
    "       \"anthropic/claude-3.5-sonnet\":\n",
    "        \"Replicate\"}\n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "vector_db_folder = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    parameters_dict,\n",
    "    results_folder=results_folder, \n",
    "    vector_db_folder=vector_db_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765bcaf5",
   "metadata": {},
   "source": [
    "# Evaluate RAG SOTA embeddings: snowflake-artic-embed-l-v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb576fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very slow - likely due to GPU/CPU memory issues\n",
    "# TODO: use cloud computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2faae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with SOTA embeddings \n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "      #  \"Snowflake/snowflake-arctic-embed-l-v2.0\": \"SentenceTransformers\" # ranked 6th, 568M params, released in december 2024 \n",
    "        \"all-MiniLM-L6-v2\": \"SentenceTransformers\"\n",
    "        },\n",
    "    \"models\": {\"gpt-3.5-turbo\": \"OpenAI\"}\n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "vector_db_folder = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    parameters_dict,\n",
    "    results_folder=results_folder, \n",
    "    vector_db_folder=vector_db_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d8d63",
   "metadata": {},
   "source": [
    "# Explore replicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows to run generative AI models in the Cloud \n",
    "# Claude-3-5-sonnet is the best llm for short context (less than 5k tokens) according to this research: https://www.galileo.ai/blog/best-llms-for-rag\n",
    "# the second one is llama-3-70b-instruct\n",
    "# TODO: try/compare other cloud providers. Try also HuggingFace inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_system_prompt = f\"\"\"\n",
    "You are a highly accurate and reliable assistant. Answer the user's question using **only** the provided context.\n",
    "If the answer is not in the context, return an empty response (**\"\"**) without making up information.\n",
    "\n",
    "Context:\n",
    "%s\n",
    "\n",
    "Instructions:\n",
    "- Answer concisely and precisely.\n",
    "- If the answer is explicitly stated in the context, extract it as-is.\n",
    "- If the answer is not in the context, return **\"\"** (empty string).\n",
    "- Do **not** infer, assume, or add external information.\n",
    "\n",
    "Example:\n",
    "    **Question:** What is the capital of Italy?\n",
    "    **Answer:** Rome\n",
    "\n",
    "Question: %s\n",
    "Answer (just the answer, no extra words, or \"\" if unknown):\n",
    "\"\"\"\n",
    "\n",
    "query = \"Who were the normans?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25697d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "output = replicate.run(\n",
    "   # \"anthropic/claude-3.5-sonnet\", \n",
    "   \"meta/meta-llama-3-70b-instruct\",\n",
    "    input={\n",
    "    \"prompt\": query,\n",
    "    \"system_prompt\": custom_system_prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\".format(system_prompt=custom_system_prompt, prompt=\"{prompt}\"),})\n",
    "\n",
    "output_merged = \"\".join(s for s in output if s not in ['\\n', '\\t', '\\r', '\"\"'])\n",
    "output_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fe36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55f29cb2",
   "metadata": {},
   "source": [
    "# Trye Qdrant vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supports hybrid search: combine sparse (ie TF-IDF, BM25) and dense vectors \n",
    "# supports reranking and more advanced search strategies for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc52219",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58611d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that if you’ve added documents with HYBRID mode, you can switch to any retrieval mode when searching. Since both the dense and sparse vectors are available in the collection.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    langchain_docs,\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    #location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    path=\"./vector_databases_qdrant/\",  # Local mode, without using the Qdrant server, may also store your vectors on disk so they’re persisted between runs.\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID, # RetrievalMode.DENSE, RetrievalMode.SPARSE\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "vector_store = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    collection_name=\"my_documents\",\n",
    "    path=\"./vector_databases_qdrant/\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID\n",
    ")\n",
    "\n",
    "\n",
    "#You can also transform the vector store into a retriever for easier usage in your chains.\n",
    "#retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "#retriever.invoke(query)\n",
    "\n",
    "query = \"Who were the normans?\"\n",
    "found_docs = vector_store.similarity_search_with_score(query, k=3)\n",
    "\n",
    "\n",
    "for doc, score in found_docs:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata filtering\n",
    "from qdrant_client import models\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=1,\n",
    "    filter=models.Filter(\n",
    "        should=[\n",
    "            models.FieldCondition(\n",
    "                key=\"page_content\",\n",
    "                match=models.MatchValue(\n",
    "                    value=\"The top 10 soccer players in the world right now.\"\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5689ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206e1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d003d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "metadata = []\n",
    "print(len(langchain_docs))\n",
    "\n",
    "for doc in langchain_docs:\n",
    "    docs.append(doc.page_content)\n",
    "    metadata.append(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "collections = client.get_collections()\n",
    "\n",
    "print(collections)\n",
    "\n",
    "collection_name = \"my_documents\"\n",
    "\n",
    "client.delete_collection(collection_name=collection_name)\n",
    "collections = client.get_collections()\n",
    "\n",
    "print(\"here\")\n",
    "print(collections)\n",
    "\n",
    "#client.set_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "client.set_model(\"snowflake/snowflake-arctic-embed-s\")\n",
    "#client.set_model(\"openai/OpenAI text-embedding-3-small\") # OpenAI Embeddings are supported but with a different workflow\n",
    "\n",
    "# comment this line to use dense vectors only\n",
    "client.set_sparse_model(\"prithivida/Splade_PP_en_v1\")#\"Qdrant/BM25\") #\"prithivida/Splade_PP_en_v1\")\n",
    "\n",
    "\n",
    "# Methods get_fastembed_vector_params and get_fastembed_sparse_vector_params help you to get the corresponding parameters for the models you are using. These parameters include vector size, distance function, etc.\n",
    "\n",
    "# Without fastembed integration, you would need to specify the vector size and distance function manually. \n",
    "if not client.collection_exists(collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=client.get_fastembed_vector_params(),\n",
    "        # comment this line to use dense vectors only\n",
    "        sparse_vectors_config=client.get_fastembed_sparse_vector_params(),  \n",
    "    )\n",
    "    client.add(\n",
    "        collection_name=collection_name,\n",
    "        documents=docs,\n",
    "        metadata=metadata,\n",
    "        parallel=0,  # Use all available CPU cores to encode data. \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if not client.collection_exists(collection_name):\n",
    "    \n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        # TODO: review vectors_config and sparse_vectors_config\n",
    "        vectors_config={\n",
    "        \"text-dense\": models.VectorParams(\n",
    "            size=1536,  # OpenAI Embeddings\n",
    "            distance=models.Distance.COSINE,\n",
    "            )\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "        \"text-sparse\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(\n",
    "                on_disk=False,\n",
    "            )\n",
    "        )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    vector_store = QdrantVectorStore.from_documents(\n",
    "        langchain_docs,\n",
    "        embedding=embeddings,\n",
    "        client=client,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        collection_name=collection_name,\n",
    "        retrieval_mode=RetrievalMode.HYBRID, # RetrievalMode.DENSE, RetrievalMode.SPARSE\n",
    "    )\n",
    "else:\n",
    "    \n",
    "    vector_store = QdrantVectorStore.from_existing_collection(\n",
    "        embedding=embeddings,\n",
    "        sparse_embedding=sparse_embeddings,\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "      #  path=\"./vector_databases_qdrant/\",\n",
    "        retrieval_mode=RetrievalMode.HYBRID\n",
    "    )\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a017ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct, CollectionStatus, UpdateStatus\n",
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "from qdrant_client.http import models\n",
    "from typing import List\n",
    "\n",
    "import openai\n",
    "#from openai.embeddings_utils import get_embedding\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"localhost\",\n",
    "    port=6333,\n",
    ")\n",
    "\n",
    "collection_name = \"qdrant_test\"\n",
    "vector_size = 1536\n",
    "vector_distance=Distance.COSINE\n",
    "\n",
    "def set_up_collection(collection_name: str, vector_size: int, vector_distance: str):\n",
    "\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=vector_size, distance=vector_distance)\n",
    "    )\n",
    "\n",
    "    collection_info = client.get_collection(collection_name=collection_name)\n",
    "\n",
    "try:\n",
    "    collection_info = client.get_collection(collection_name=collection_name)\n",
    "except Exception as e:\n",
    "    print(\"Collection does not exist, creating collection now\")\n",
    "    set_up_collection(collection_name,  vector_size, vector_distance)\n",
    "\n",
    "embeddings_model_name = \"text-embedding-3-small\"\n",
    "\n",
    "embeddings_function = OpenAIEmbeddings(\n",
    "    model=embeddings_model_name\n",
    ")\n",
    "\n",
    "def upsert_data(data: List, client):\n",
    "    from openai import OpenAI\n",
    "    openaiclient = OpenAI()\n",
    "    points = []\n",
    "    for doc in data:\n",
    "       # quote = item.get(\"quote\")\n",
    "        #person = item.get(\"person\")\n",
    "\n",
    "        text_vector = openaiclient.embeddings.create(input = [doc], model=embeddings_model_name).data[0].embedding #embeddings_function.encode(doc) #get_embedding(quote, engine=\"text-embedding-ada-002\")\n",
    "        text_id = str(uuid.uuid4())\n",
    "        #payload = {\"quote\": quote, \"person\": person}\n",
    "        point = PointStruct(id=text_id, vector=text_vector) #payload=payload)\n",
    "        points.append(point)\n",
    "\n",
    "    operation_info = client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        wait=True,\n",
    "        points=points)\n",
    "\n",
    "    if operation_info.status == UpdateStatus.COMPLETED:\n",
    "        print(\"Data inserted successfully!\")\n",
    "    else:\n",
    "        print(\"Failed to insert data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_data(docs[0:10], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5764c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802310fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40888a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31dd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(input_query: str, limit: int = 3):\n",
    "    from openai import OpenAI\n",
    "    openaiclient = OpenAI()\n",
    "    embeddings_model = \"text-embedding-3-small\"\n",
    "    input_vector = openaiclient.embeddings.create(input = [input_query], model=embeddings_model).data[0].embedding #embeddings_function.encode(doc) #get_embedding(quote, engine=\"text-embedding-a#get_embedding(input_query, engine=\"text-embedding-ada-002\")\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=input_vector,\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "    print(\"search results:\")\n",
    "    print(search_result)\n",
    "\n",
    "    result = []\n",
    "    for item in search_result:\n",
    "        similarity_score = item.score\n",
    "        payload = item.payload\n",
    "        data = {\"id\": item.id, \"similarity_score\": similarity_score, \"quote\": payload.get(\"quote\"), \"person\": payload.get(\"person\")}\n",
    "        result.append(data)\n",
    "\n",
    "    return result\n",
    "\n",
    "result = search(\"who were the normans?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476af2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47e17c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection exists! Skipping creation!\n",
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=10 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1536, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfig(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, filter_max_conditions=None, condition_max_size=None)) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "from vector_store import QdrantVectorStore\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"qdrant\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-small\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "\n",
    "qdrant_vs = QdrantVectorStore(knowledge_base=langchain_docs, config_dict=parameters_dict)\n",
    "qdrant_vs.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a29e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the collection\n",
      "Collections before deletion:  collections=[CollectionDescription(name='my_documents'), CollectionDescription(name='qdrant_test'), CollectionDescription(name='400_text-embedding-3-small'), CollectionDescription(name='400_text-embedding-3-large')]\n",
      "Collections after deletion:  collections=[CollectionDescription(name='my_documents'), CollectionDescription(name='qdrant_test'), CollectionDescription(name='400_text-embedding-3-large')]\n"
     ]
    }
   ],
   "source": [
    "qdrant_vs.delete_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de0a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection does not exist, creating collection now\n",
      "Adding data in the database\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PointStruct\ncontext\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'context': 'The Normans ... succeeding centuries.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mqdrant_vs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Git Projects/RAG-Optimization/vector_store.py:93\u001b[39m, in \u001b[36mQdrantVectorStore.create_vector_store\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m.client.recreate_collection(\n\u001b[32m     88\u001b[39m         collection_name=\u001b[38;5;28mself\u001b[39m.vector_database_name,\n\u001b[32m     89\u001b[39m         vectors_config=VectorParams(size=\u001b[38;5;28mself\u001b[39m.vector_size, distance=\u001b[38;5;28mself\u001b[39m.vector_distance),\n\u001b[32m     90\u001b[39m        \u001b[38;5;66;03m# sparse_vectors_config={\"text\": models.SparseVectorParams()} # TODO\u001b[39;00m\n\u001b[32m     91\u001b[39m     )\n\u001b[32m     92\u001b[39m     collection_info = \u001b[38;5;28mself\u001b[39m.client.get_collection(collection_name=\u001b[38;5;28mself\u001b[39m.vector_database_name)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupsert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(collection_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Git Projects/RAG-Optimization/vector_store.py:114\u001b[39m, in \u001b[36mQdrantVectorStore.upsert_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    112\u001b[39m     text_id = \u001b[38;5;28mstr\u001b[39m(uuid.uuid4())\n\u001b[32m    113\u001b[39m     context = {\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: doc}\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     point = \u001b[43mPointStruct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtext_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     points.append(point)\n\u001b[32m    117\u001b[39m operation_info = \u001b[38;5;28mself\u001b[39m.client.upsert(\n\u001b[32m    118\u001b[39m     collection_name=\u001b[38;5;28mself\u001b[39m.vector_database_name,\n\u001b[32m    119\u001b[39m     wait=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    120\u001b[39m     points=points\n\u001b[32m    121\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Git Projects/RAG-Optimization/.venv/lib/python3.12/site-packages/pydantic/main.py:214\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    213\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    216\u001b[39m     warnings.warn(\n\u001b[32m    217\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    218\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    220\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for PointStruct\ncontext\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'context': 'The Normans ... succeeding centuries.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "qdrant_vs.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97249ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='5a40b8dc-65ed-49e8-ba75-f05a8a57ac54', version=0, score=0.7553296, payload={}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='8eb1623c-4bd1-4117-91de-33f0577df8d6', version=0, score=0.70617324, payload={}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='22d0fbc8-db66-4344-b45d-9fd0c6275f23', version=0, score=0.6593559, payload={}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = qdrant_vs.query_vector_store(\"normans\")\n",
    "context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
