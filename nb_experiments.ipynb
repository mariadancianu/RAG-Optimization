{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0accd0d-fb0b-4c5e-be02-8063d3b6a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariadancianu/Desktop/Git Projects/RAG-Optimization/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from rag_optimization import convert_knowledge_base_to_langchain_docs, optimize_rag_parameters\n",
    "from data_utils import convert_json_to_dataframe, create_json_subset, collect_all_results, merge_results\n",
    "sns.set_style(\"whitegrid\")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful material \n",
    "# SQuAD Evaluation guidelines: \n",
    "# https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/\n",
    "# https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b749b",
   "metadata": {},
   "source": [
    "# Convert json data to pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d8df1-cea0-410b-9c27-985406ea208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_json_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = pd.read_csv(\"dataset.csv\")\n",
    "df_all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771778e-ae9e-49b4-9216-658bdefc8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2eebd-70b9-4118-b691-0c6c96e72a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some rough statistics for the context length \n",
    "df_selected.loc[:, \"context_chars\"] = df_selected[\"context\"].apply(lambda x: len(x))\n",
    "df_selected.loc[:, \"context_words\"] = df_selected.loc[:, \"context\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f66f8e-3eb9-4943-9b2e-40df68068a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cab993-b3ab-4947-acb5-016382d1321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "sns.histplot(df_selected.drop_duplicates(subset=\"context\")[\"context_chars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb1095-e0d8-4f06-b203-08ff1689aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "sns.histplot(df_selected.drop_duplicates(subset=\"context\")[\"context_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9848fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create original json structure for only a subset of questions, used for tests and fine-tuning \n",
    "# this file will be used by the evaluation.py file \n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_sel = df[0:500]\n",
    "df_sel.head(2)\n",
    "\n",
    "create_json_subset(df_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c8057-750e-44d6-81c8-d8ad6717b311",
   "metadata": {},
   "source": [
    "# RAG architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf8664-3381-4701-b297-bbcfa5176872",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "**Data Indexing**\n",
    "\n",
    "Converting text data into a searchable database of vector embeddings, which represent the meaning of the text in a format that computers can easily understand.\n",
    "- **Documents Chunking**: The collection of documents is split into smaller chunks of text. This allows for more precise and relevant pieces of information to be fed into the language model when needed, avoiding information overload.\n",
    "- **Vector Embeddings**: The chunks of text are then transformed into vector embeddings. These embeddings encode the meaning of natural language text into numerical representations.\n",
    "- **Vector Database**: Finally, the vector embeddings are stored in a vector database, making them easily searchable.\n",
    "\n",
    "**Documents -> Text chunks -> Vector Embeddings -> Vector DB**\n",
    "\n",
    "**Load -> Split -> Embed -> Store**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfc599",
   "metadata": {},
   "source": [
    "## Convert the pandas context to Langchain documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f241f-d54c-452e-94ad-cfcc8a81a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8e92a-9322-4359-b18b-f4217fb78003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(langchain_docs))\n",
    "print(langchain_docs[0])\n",
    "print(langchain_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2075d54",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_optimization import CustomRAG, prompt_message, convert_knowledge_base_to_langchain_docs\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"chromadb\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-large\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "    \"llm\": {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"client\": \"OpenAI\"\n",
    "        }\n",
    "}\n",
    "\n",
    "df_to_test = df[0:10]\n",
    "\n",
    "rag = CustomRAG(knowledge_base=langchain_docs, \n",
    "                prompt_message=prompt_message,\n",
    "                config_dict=parameters_dict, \n",
    "                results_folder='./eval_results/test_new_class', \n",
    "                vector_db_folder='./vector_databases/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.vector_store.initialize_embeddings_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9878b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = \"text-embedding-3-small\"\n",
    "embeddings = OpenAIEmbeddings(model=embeddings_model)\n",
    "\n",
    "db_dir = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "rag.vector_store.create_vector_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0571139",
   "metadata": {},
   "source": [
    "## Querying the vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5fcd1-6677-4a48-823c-7b899bfee9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is the weather today in Milan?\"\n",
    "relevant_docs = rag.vector_store.query_vector_store(query, n_results=3, score_threshold=0.1)\n",
    "\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who were the normans?\"\n",
    "context = rag.vector_store.query_vector_store(query, n_results=3, score_threshold=0.1)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5aa673",
   "metadata": {},
   "source": [
    "## Run the RAG over a subset of questions and save the answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_test = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df_to_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff098d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag.get_llm_multiple_questions_answers(df_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204b4fa",
   "metadata": {},
   "source": [
    "## RAG Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273535f0",
   "metadata": {},
   "source": [
    "TEXT CHUNKING \n",
    "\n",
    "1. CHARACTER SPLITTING : divide the text into N-character sized chunks. Can split words in the middle. \n",
    "2. RECURSIVE CHARACTER SPLITTING: preserves sentences. Avoids splitting sentences midword (note that RecursiveCharacterTextSplitter with separator does exactly that). Split the\n",
    "document where a double new line is present, then, if the chunk size is still exceeded, split at new lines, and so on.\n",
    "3. SEMANTIC SPLITTING: keeps related content together. Use embeddings to split based on meaning.\n",
    "+ other techniques\n",
    "\n",
    "EMBEDDINGS \n",
    "Create fixed-length vector representation of text, focusing on semanting meaning for tasks like similarity comparison. \n",
    "Most up to date embedding models, both proprietary and open source, with performance metrics across different tasks: https://huggingface.co/spaces/mteb/leaderboard.\n",
    "\n",
    "This contains also a \"retrieval\" column with performance metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0830832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    results_folder=\"eval_results/test_new_class\", \n",
    "    vector_db_folder=\"vector_databases/test_new_class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"eval_results/optimize_results\"\n",
    "df_all_res = collect_all_results(path)\n",
    "df_all_res.sort_values(by=\"HasAns_f1\", ascending=False, inplace=True)\n",
    "df_all_res.to_csv(f\"{path}/df_all_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c923406",
   "metadata": {},
   "source": [
    "# Investigate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008528cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best results and merge the scores by question id to the original df in order to inspect the errors.\n",
    "# The idea is to understand why the results are so poor for the NoAns questions, when the HasAns questions have \n",
    "# a high f1 score, in order to understand how the workflow can be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeaa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_path = os.path.join(os.getcwd(), \"eval_results/initial_eval_results\", df_all_res.experiment.iloc[0])\n",
    "split_path = best_result_path.split(\"/\")\n",
    "split_path[-1] = split_path[-1].replace(\"eval_\", \"\")\n",
    "best_result_path = \"/\".join(split_path)\n",
    "best_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1beec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python \"$(pwd)/eval_results/evaluation.py\" \"$(pwd)/eval_results/data_updated_500.json\" \"$(pwd)/eval_results/optimize_results/pred_400_all-MiniLM-L6-v2_gpt-3.5-turbo.json\" --out-file \"$(pwd)/eval_results/optimize_results/eval_pred_400_all-MiniLM-L6-v2_gpt-3.5-turbo.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_results(f1_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/f1_thresh_by_qid.json\"), \n",
    "                          exact_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/exact_thresh_by_qid.json\"), \n",
    "                          pred_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"), \n",
    "                          filepath_500=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"),\n",
    "                          context_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/context_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"), \n",
    "                          df_questions_filepath=\"dataset.csv\", \n",
    "                          filter_500=True)\n",
    "\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585095c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9391a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.is_impossible][[\"id\", \"is_impossible\", \"f1_score\", \"exact_score\", \"question\", \"pred\"]].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.loc[479, \"context\"].replace(\". \", \".\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.loc[479, \"rag_retrieved_context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cef210",
   "metadata": {},
   "source": [
    "# Evaluate with other LLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with best parameters, and save also the context\n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "        \"text-embedding-3-small\": \"OpenAI\", \n",
    "        },\n",
    "    \"models\": {\n",
    "       # \"meta/meta-llama-3-70b-instruct\":\n",
    "       \"anthropic/claude-3.5-sonnet\":\n",
    "        \"Replicate\"}\n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "vector_db_folder = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    parameters_dict,\n",
    "    results_folder=results_folder, \n",
    "    vector_db_folder=vector_db_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765bcaf5",
   "metadata": {},
   "source": [
    "# Evaluate RAG SOTA embeddings: snowflake-artic-embed-l-v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb576fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very slow - likely due to GPU/CPU memory issues\n",
    "# TODO: use cloud computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2faae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with SOTA embeddings \n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "      #  \"Snowflake/snowflake-arctic-embed-l-v2.0\": \"SentenceTransformers\" # ranked 6th, 568M params, released in december 2024 \n",
    "        \"all-MiniLM-L6-v2\": \"SentenceTransformers\"\n",
    "        },\n",
    "    \"models\": {\"gpt-3.5-turbo\": \"OpenAI\"}\n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "vector_db_folder = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    parameters_dict,\n",
    "    results_folder=results_folder, \n",
    "    vector_db_folder=vector_db_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d8d63",
   "metadata": {},
   "source": [
    "# Explore replicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows to run generative AI models in the Cloud \n",
    "# Claude-3-5-sonnet is the best llm for short context (less than 5k tokens) according to this research: https://www.galileo.ai/blog/best-llms-for-rag\n",
    "# the second one is llama-3-70b-instruct\n",
    "# TODO: try/compare other cloud providers. Try also HuggingFace inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_system_prompt = f\"\"\"\n",
    "You are a highly accurate and reliable assistant. Answer the user's question using **only** the provided context.\n",
    "If the answer is not in the context, return an empty response (**\"\"**) without making up information.\n",
    "\n",
    "Context:\n",
    "%s\n",
    "\n",
    "Instructions:\n",
    "- Answer concisely and precisely.\n",
    "- If the answer is explicitly stated in the context, extract it as-is.\n",
    "- If the answer is not in the context, return **\"\"** (empty string).\n",
    "- Do **not** infer, assume, or add external information.\n",
    "\n",
    "Example:\n",
    "    **Question:** What is the capital of Italy?\n",
    "    **Answer:** Rome\n",
    "\n",
    "Question: %s\n",
    "Answer (just the answer, no extra words, or \"\" if unknown):\n",
    "\"\"\"\n",
    "\n",
    "query = \"Who were the normans?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25697d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "output = replicate.run(\n",
    "   # \"anthropic/claude-3.5-sonnet\", \n",
    "   \"meta/meta-llama-3-70b-instruct\",\n",
    "    input={\n",
    "    \"prompt\": query,\n",
    "    \"system_prompt\": custom_system_prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\".format(system_prompt=custom_system_prompt, prompt=\"{prompt}\"),})\n",
    "\n",
    "output_merged = \"\".join(s for s in output if s not in ['\\n', '\\t', '\\r', '\"\"'])\n",
    "output_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fe36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55f29cb2",
   "metadata": {},
   "source": [
    "# Trye Qdrant vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supports hybrid search: combine sparse (ie TF-IDF, BM25) and dense vectors \n",
    "# supports reranking and more advanced search strategies for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d63ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_store import QdrantVectorStore\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[5937:]\n",
    "df.index.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"qdrant\",\n",
    "    \"sparse_text_model\": \"Qdrant/bm25\",\n",
    "    \n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-small\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "\n",
    "qdrant_vs = QdrantVectorStore(knowledge_base=langchain_docs, config_dict=parameters_dict)\n",
    "qdrant_vs.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989190ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the content of the vector datbae\n",
    "res = qdrant_vs.client.scroll(\n",
    "    collection_name=qdrant_vs.vector_database_name,\n",
    "    limit=1,\n",
    "    with_payload=True,\n",
    "    with_vectors=True,\n",
    ")\n",
    "\n",
    "res[0][0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_vs.delete_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97249ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = qdrant_vs.query_vector_store(\"normans\")\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785837ea",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9876f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from retrieval_evaluation import RetrievalEvaluator\n",
    "from vector_store import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"chromadb\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-small\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "\n",
    "vector_store = ChromaVectorStore(knowledge_base=langchain_docs, \n",
    "                                 config_dict=parameters_dict, \n",
    "                                 vector_db_folder='./vector_databases/')\n",
    "\n",
    "vector_store.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddb904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"answer_0\", \"answer_1\", \"answer_2\", \"answer_3\", \"is_impossible\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = df.loc[0:500]\n",
    "\n",
    "evaluator = RetrievalEvaluator(df_ground_truth=df_ground_truth, \n",
    "                               vector_store=vector_store,\n",
    "                               path_to_results=\"./eval_results/retrieval_eval/\")\n",
    "\n",
    "results_dict = evaluator.evaluate()\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2800498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection exists! Skipping creation!\n",
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=588 points_count=588 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors={'text-dense': VectorParams(size=1536, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}, shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors={'text-sparse': SparseVectorParams(index=SparseIndexParams(full_scan_threshold=None, on_disk=False, datatype=None), modifier=None)}), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfig(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, filter_max_conditions=None, condition_max_size=None)) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "from vector_store import QdrantVectorStore\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"qdrant\",\n",
    "    \"sparse_text_model\": \"Qdrant/bm25\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-small\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "\n",
    "qdrant_vs = QdrantVectorStore(knowledge_base=langchain_docs, config_dict=parameters_dict)\n",
    "qdrant_vs.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2f167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [01:43,  2.28it/s]"
     ]
    }
   ],
   "source": [
    "df_ground_truth = df.loc[0:500] #500]\n",
    "\n",
    "evaluator = RetrievalEvaluator(df_ground_truth=df_ground_truth, \n",
    "                               vector_store=qdrant_vs,\n",
    "                               path_to_results=\"./eval_results/retrieval_eval/\")\n",
    "\n",
    "results_dict = evaluator.evaluate()\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a9654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee191acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c17d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_0</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>mrr_score</th>\n",
       "      <th>context_0</th>\n",
       "      <th>context_1</th>\n",
       "      <th>context_2</th>\n",
       "      <th>context</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>relevant_chunks</th>\n",
       "      <th>retrieved_relevant_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>Construction is the process of constructing a ...</td>\n",
       "      <td>Normandy was the site of several important dev...</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{0}]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question answer_0 answer_1 answer_2 answer_3  \\\n",
       "0  In what country is Normandy located?   France   France   France   France   \n",
       "\n",
       "   mrr_score                                          context_0  \\\n",
       "0        1.0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                                           context_1  \\\n",
       "0  Construction is the process of constructing a ...   \n",
       "\n",
       "                                           context_2  \\\n",
       "0  Normandy was the site of several important dev...   \n",
       "\n",
       "                                             context  precision_score  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...         0.333333   \n",
       "\n",
       "   recall_score relevant_chunks  retrieved_relevant_chunks  \n",
       "0           1.0           [{0}]                        1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./eval_results/retrieval_eval/qdrant_400_15_text-embedding-3-small_evaluation_results.csv\")\n",
    "df[[\"question\", \"answer_0\", \"answer_1\", \"answer_2\", \"answer_3\", \"mrr_score\", \n",
    "    \"context_0\", \"context_1\", \"context_2\", \"context\",\n",
    "    \"precision_score\", \"recall_score\", \"relevant_chunks\", \"retrieved_relevant_chunks\"]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f26674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>relevant_chunks</th>\n",
       "      <th>retrieved_relevant_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [precision_score, recall_score, relevant_chunks, retrieved_relevant_chunks]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.is_impossible][[\"precision_score\", \"recall_score\", \"relevant_chunks\", \"retrieved_relevant_chunks\"]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f7e858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"context_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d344d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Construction is the process of constructing a building or infrastructure. Construction differs from manufacturing in that manufacturing typically involves mass production of similar items without a designated purchaser, while construction typically takes place on location for a known client. Construction as an industry comprises six to nine percent of the gross domestic product of developed countries. Construction starts with planning,[citation needed] design, and financing and continues until the project is built and ready for use.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"context_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b00d6963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Normandy was the site of several important developments in the history of classical music in the 11th century. Fécamp Abbey and Saint-Evroul Abbey were centres of musical production and education. At Fécamp, under two Italian abbots, William of Volpiano and John of Ravenna, the system of denoting notes by letters was developed and taught. It is still the most common form of pitch representation in English- and German-speaking countries today. Also at Fécamp, the staff, around which neumes were oriented, was first developed and taught in the 11th century. Under the German abbot Isembard, La Trinité-du-Mont became a centre of musical composition.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, \"context_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be297df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Qdrant with different embeddings \n",
    "from vector_store import QdrantVectorStore\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "langchain_docs = langchain_docs[:len(langchain_docs)/2]\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"qdrant\",\n",
    "    \"sparse_text_model\": \"Qdrant/bm25\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-small\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "\n",
    "qdrant_vs = QdrantVectorStore(knowledge_base=langchain_docs, config_dict=parameters_dict)\n",
    "qdrant_vs.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57939013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455ace10",
   "metadata": {},
   "source": [
    "# Run rag optimization workflow with qdrant and claude-3.5-sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    #\"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"qdrant\",\n",
    "    \"sparse_text_model\": \"Qdrant/bm25\",\n",
    "    \"models\": {\n",
    "       # \"meta/meta-llama-3-70b-instruct\":\n",
    "       \"anthropic/claude-3.5-sonnet\": \"Replicate\"},\n",
    "    \"embed_options\": {\n",
    "        \"text-embedding-3-small\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "vector_db_folder = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    parameters_dict,\n",
    "    results_folder=results_folder, \n",
    "    vector_db_folder=vector_db_folder\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
