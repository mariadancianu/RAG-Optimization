{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0accd0d-fb0b-4c5e-be02-8063d3b6a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariadancianu/Desktop/Git Projects/RAG-Optimization/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from rag_optimization import convert_knowledge_base_to_langchain_docs, optimize_rag_parameters\n",
    "from data_utils import convert_json_to_dataframe, create_json_subset, collect_all_results, merge_results\n",
    "sns.set_style(\"whitegrid\")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953814b1",
   "metadata": {},
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful material \n",
    "# SQuAD Evaluation guidelines: \n",
    "# https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/\n",
    "# https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b749b",
   "metadata": {},
   "source": [
    "# Convert json data to pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d8df1-cea0-410b-9c27-985406ea208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_json_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = pd.read_csv(\"dataset.csv\")\n",
    "df_all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771778e-ae9e-49b4-9216-658bdefc8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2eebd-70b9-4118-b691-0c6c96e72a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some rough statistics for the context length \n",
    "df_selected.loc[:, \"context_chars\"] = df_selected[\"context\"].apply(lambda x: len(x))\n",
    "df_selected.loc[:, \"context_words\"] = df_selected.loc[:, \"context\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f66f8e-3eb9-4943-9b2e-40df68068a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cab993-b3ab-4947-acb5-016382d1321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "sns.histplot(df_selected.drop_duplicates(subset=\"context\")[\"context_chars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb1095-e0d8-4f06-b203-08ff1689aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "sns.histplot(df_selected.drop_duplicates(subset=\"context\")[\"context_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9848fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create original json structure for only a subset of questions, used for tests and fine-tuning \n",
    "# this file will be used by the evaluation.py file \n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_sel = df[0:500]\n",
    "df_sel.head(2)\n",
    "\n",
    "create_json_subset(df_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c8057-750e-44d6-81c8-d8ad6717b311",
   "metadata": {},
   "source": [
    "# RAG architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf8664-3381-4701-b297-bbcfa5176872",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "**Data Indexing**\n",
    "\n",
    "Converting text data into a searchable database of vector embeddings, which represent the meaning of the text in a format that computers can easily understand.\n",
    "- **Documents Chunking**: The collection of documents is split into smaller chunks of text. This allows for more precise and relevant pieces of information to be fed into the language model when needed, avoiding information overload.\n",
    "- **Vector Embeddings**: The chunks of text are then transformed into vector embeddings. These embeddings encode the meaning of natural language text into numerical representations.\n",
    "- **Vector Database**: Finally, the vector embeddings are stored in a vector database, making them easily searchable.\n",
    "\n",
    "**Documents -> Text chunks -> Vector Embeddings -> Vector DB**\n",
    "\n",
    "**Load -> Split -> Embed -> Store**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfc599",
   "metadata": {},
   "source": [
    "## Convert the pandas context to Langchain documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f241f-d54c-452e-94ad-cfcc8a81a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8e92a-9322-4359-b18b-f4217fb78003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(langchain_docs))\n",
    "print(langchain_docs[0])\n",
    "print(langchain_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2075d54",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_optimization import CustomRAG, prompt_message, convert_knowledge_base_to_langchain_docs\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"chromadb\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-large\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "    \"llm\": {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"client\": \"OpenAI\"\n",
    "        }\n",
    "}\n",
    "\n",
    "df_to_test = df[0:10]\n",
    "\n",
    "rag = CustomRAG(knowledge_base=langchain_docs, \n",
    "                prompt_message=prompt_message,\n",
    "                config_dict=parameters_dict, \n",
    "                results_folder='./eval_results/test_new_class', \n",
    "                vector_db_folder='./vector_databases/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.vector_store.initialize_embeddings_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9878b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = \"text-embedding-3-small\"\n",
    "embeddings = OpenAIEmbeddings(model=embeddings_model)\n",
    "\n",
    "db_dir = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "rag.vector_store.create_vector_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0571139",
   "metadata": {},
   "source": [
    "## Querying the vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5fcd1-6677-4a48-823c-7b899bfee9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is the weather today in Milan?\"\n",
    "relevant_docs = rag.vector_store.query_vector_store(query, n_results=3, score_threshold=0.1)\n",
    "\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who were the normans?\"\n",
    "context = rag.vector_store.query_vector_store(query, n_results=3, score_threshold=0.1)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5aa673",
   "metadata": {},
   "source": [
    "## Run the RAG over a subset of questions and save the answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_test = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df_to_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff098d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag.get_llm_multiple_questions_answers(df_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204b4fa",
   "metadata": {},
   "source": [
    "## RAG Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273535f0",
   "metadata": {},
   "source": [
    "TEXT CHUNKING \n",
    "\n",
    "1. CHARACTER SPLITTING : divide the text into N-character sized chunks. Can split words in the middle. \n",
    "2. RECURSIVE CHARACTER SPLITTING: preserves sentences. Avoids splitting sentences midword (note that RecursiveCharacterTextSplitter with separator does exactly that). Split the\n",
    "document where a double new line is present, then, if the chunk size is still exceeded, split at new lines, and so on.\n",
    "3. SEMANTIC SPLITTING: keeps related content together. Use embeddings to split based on meaning.\n",
    "+ other techniques\n",
    "\n",
    "EMBEDDINGS \n",
    "Create fixed-length vector representation of text, focusing on semanting meaning for tasks like similarity comparison. \n",
    "Most up to date embedding models, both proprietary and open source, with performance metrics across different tasks: https://huggingface.co/spaces/mteb/leaderboard.\n",
    "\n",
    "This contains also a \"retrieval\" column with performance metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0830832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    results_folder=\"eval_results/test_new_class\", \n",
    "    vector_db_folder=\"vector_databases/test_new_class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"eval_results/optimize_results\"\n",
    "df_all_res = collect_all_results(path)\n",
    "df_all_res.sort_values(by=\"HasAns_f1\", ascending=False, inplace=True)\n",
    "df_all_res.to_csv(f\"{path}/df_all_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c923406",
   "metadata": {},
   "source": [
    "# Investigate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008528cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best results and merge the scores by question id to the original df in order to inspect the errors.\n",
    "# The idea is to understand why the results are so poor for the NoAns questions, when the HasAns questions have \n",
    "# a high f1 score, in order to understand how the workflow can be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeaa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_path = os.path.join(os.getcwd(), \"eval_results/initial_eval_results\", df_all_res.experiment.iloc[0])\n",
    "split_path = best_result_path.split(\"/\")\n",
    "split_path[-1] = split_path[-1].replace(\"eval_\", \"\")\n",
    "best_result_path = \"/\".join(split_path)\n",
    "best_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1beec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python \"$(pwd)/eval_results/evaluation.py\" \"$(pwd)/eval_results/data_updated_500.json\" \"$(pwd)/eval_results/optimize_results/pred_400_all-MiniLM-L6-v2_gpt-3.5-turbo.json\" --out-file \"$(pwd)/eval_results/optimize_results/eval_pred_400_all-MiniLM-L6-v2_gpt-3.5-turbo.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_results(f1_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/f1_thresh_by_qid.json\"), \n",
    "                          exact_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/exact_thresh_by_qid.json\"), \n",
    "                          pred_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"), \n",
    "                          filepath_500=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"),\n",
    "                          context_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/context_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"), \n",
    "                          df_questions_filepath=\"dataset.csv\", \n",
    "                          filter_500=True)\n",
    "\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585095c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9391a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.is_impossible][[\"id\", \"is_impossible\", \"f1_score\", \"exact_score\", \"question\", \"pred\"]].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.loc[479, \"context\"].replace(\". \", \".\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.loc[479, \"rag_retrieved_context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cef210",
   "metadata": {},
   "source": [
    "# Evaluate with other LLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with best parameters, and save also the context\n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "        \"text-embedding-3-small\": \"OpenAI\", \n",
    "        },\n",
    "    \"models\": {\n",
    "       # \"meta/meta-llama-3-70b-instruct\":\n",
    "       \"anthropic/claude-3.5-sonnet\":\n",
    "        \"Replicate\"}\n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "vector_db_folder = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    parameters_dict,\n",
    "    results_folder=results_folder, \n",
    "    vector_db_folder=vector_db_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765bcaf5",
   "metadata": {},
   "source": [
    "# Evaluate RAG SOTA embeddings: snowflake-artic-embed-l-v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb576fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very slow - likely due to GPU/CPU memory issues\n",
    "# TODO: use cloud computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2faae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with SOTA embeddings \n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "      #  \"Snowflake/snowflake-arctic-embed-l-v2.0\": \"SentenceTransformers\" # ranked 6th, 568M params, released in december 2024 \n",
    "        \"all-MiniLM-L6-v2\": \"SentenceTransformers\"\n",
    "        },\n",
    "    \"models\": {\"gpt-3.5-turbo\": \"OpenAI\"}\n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "vector_db_folder = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    parameters_dict,\n",
    "    results_folder=results_folder, \n",
    "    vector_db_folder=vector_db_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d8d63",
   "metadata": {},
   "source": [
    "# Explore replicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows to run generative AI models in the Cloud \n",
    "# Claude-3-5-sonnet is the best llm for short context (less than 5k tokens) according to this research: https://www.galileo.ai/blog/best-llms-for-rag\n",
    "# the second one is llama-3-70b-instruct\n",
    "# TODO: try/compare other cloud providers. Try also HuggingFace inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_system_prompt = f\"\"\"\n",
    "You are a highly accurate and reliable assistant. Answer the user's question using **only** the provided context.\n",
    "If the answer is not in the context, return an empty response (**\"\"**) without making up information.\n",
    "\n",
    "Context:\n",
    "%s\n",
    "\n",
    "Instructions:\n",
    "- Answer concisely and precisely.\n",
    "- If the answer is explicitly stated in the context, extract it as-is.\n",
    "- If the answer is not in the context, return **\"\"** (empty string).\n",
    "- Do **not** infer, assume, or add external information.\n",
    "\n",
    "Example:\n",
    "    **Question:** What is the capital of Italy?\n",
    "    **Answer:** Rome\n",
    "\n",
    "Question: %s\n",
    "Answer (just the answer, no extra words, or \"\" if unknown):\n",
    "\"\"\"\n",
    "\n",
    "query = \"Who were the normans?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25697d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "output = replicate.run(\n",
    "   # \"anthropic/claude-3.5-sonnet\", \n",
    "   \"meta/meta-llama-3-70b-instruct\",\n",
    "    input={\n",
    "    \"prompt\": query,\n",
    "    \"system_prompt\": custom_system_prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\".format(system_prompt=custom_system_prompt, prompt=\"{prompt}\"),})\n",
    "\n",
    "output_merged = \"\".join(s for s in output if s not in ['\\n', '\\t', '\\r', '\"\"'])\n",
    "output_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fe36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55f29cb2",
   "metadata": {},
   "source": [
    "# Trye Qdrant vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supports hybrid search: combine sparse (ie TF-IDF, BM25) and dense vectors \n",
    "# supports reranking and more advanced search strategies for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d63ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_store import QdrantVectorStore\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[5937:]\n",
    "df.index.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"qdrant\",\n",
    "    \"sparse_text_model\": \"Qdrant/bm25\",\n",
    "    \n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-small\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "\n",
    "qdrant_vs = QdrantVectorStore(knowledge_base=langchain_docs, config_dict=parameters_dict)\n",
    "qdrant_vs.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989190ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the content of the vector datbae\n",
    "res = qdrant_vs.client.scroll(\n",
    "    collection_name=qdrant_vs.vector_database_name,\n",
    "    limit=1,\n",
    "    with_payload=True,\n",
    "    with_vectors=True,\n",
    ")\n",
    "\n",
    "res[0][0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_vs.delete_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97249ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = qdrant_vs.query_vector_store(\"normans\")\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785837ea",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval_evaluation import RetrievalEvaluator\n",
    "from vector_store import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"chromadb\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-small\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "\n",
    "vector_store = ChromaVectorStore(knowledge_base=langchain_docs, \n",
    "                                 config_dict=parameters_dict, \n",
    "                                 vector_db_folder='./vector_databases/')\n",
    "\n",
    "vector_store.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddb904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"answer_0\", \"answer_1\", \"answer_2\", \"answer_3\", \"is_impossible\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = df.loc[0:500]\n",
    "\n",
    "evaluator = RetrievalEvaluator(df_ground_truth=df_ground_truth, \n",
    "                               vector_store=vector_store,\n",
    "                               path_to_results=\"./eval_results/retrieval_eval/\")\n",
    "\n",
    "results_dict = evaluator.evaluate()\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2800498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_store import QdrantVectorStore\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"qdrant\",\n",
    "    \"sparse_text_model\": \"Qdrant/bm25\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-small\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "\n",
    "qdrant_vs = QdrantVectorStore(knowledge_base=langchain_docs, config_dict=parameters_dict)\n",
    "qdrant_vs.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4fea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = df.loc[0:500]\n",
    "\n",
    "evaluator = RetrievalEvaluator(df_ground_truth=df_ground_truth, \n",
    "                               vector_store=qdrant_vs,\n",
    "                               path_to_results=\"./eval_results/retrieval_eval/\")\n",
    "\n",
    "results_dict = evaluator.evaluate()\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c17d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455ace10",
   "metadata": {},
   "source": [
    "# Run rag optimization workflow with qdrant and claude-3.5-sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning RAG with the following parameters: \n",
      "{'chunk_sizes': [400],\n",
      " 'embed_options': {'model_name': 'text-embedding-3-small',\n",
      "                   'platform': 'OpenAI'},\n",
      " 'models': {'anthropic/claude-3.5-sonnet': 'Replicate'},\n",
      " 'sparse_text_model': 'Qdrant/bm25',\n",
      " 'vector_database': 'qdrant'}\n",
      "Running anthropic/claude-3.5-sonnet - 400 - model_name\n",
      "(\"CustomRAG config: {'chunk_size': 400, 'chunk_overlap': 15, \"\n",
      " \"'vector_database': 'qdrant', 'sparse_text_model': 'Qdrant/bm25', \"\n",
      " \"'embeddings_function': {'model_name': 'model_name', 'platform': \"\n",
      " \"'text-embedding-3-small'}, 'llm': {'model_name': \"\n",
      " \"'anthropic/claude-3.5-sonnet', 'client': 'Replicate'}}\")\n",
      "Using qdrant!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QdrantVectorStore' object has no attribute 'embeddings_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m df_to_test = df[\u001b[32m0\u001b[39m:\u001b[32m500\u001b[39m]\n\u001b[32m     24\u001b[39m langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43moptimize_rag_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_to_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlangchain_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvector_db_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_db_folder\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Git Projects/RAG-Optimization/rag_optimization.py:375\u001b[39m, in \u001b[36moptimize_rag_parameters\u001b[39m\u001b[34m(df, knowledge_base_docs, parameters_dict, results_folder, vector_db_folder)\u001b[39m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    363\u001b[39m parameters_dict = {\n\u001b[32m    364\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchunk_size\u001b[39m\u001b[33m\"\u001b[39m: chunk_size,\n\u001b[32m    365\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchunk_overlap\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m15\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m: model_name, \u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m\"\u001b[39m: client},\n\u001b[32m    373\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m rag = \u001b[43mCustomRAG\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mknowledge_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mknowledge_base_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvector_db_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_db_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m rag.get_llm_multiple_questions_answers(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Git Projects/RAG-Optimization/rag_optimization.py:126\u001b[39m, in \u001b[36mCustomRAG.__init__\u001b[39m\u001b[34m(self, knowledge_base, prompt_message, config_dict, results_folder, vector_db_folder, save_results)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mvector_database\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mqdrant\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing qdrant!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28mself\u001b[39m.vector_store = \u001b[43mQdrantVectorStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mknowledge_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mknowledge_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_dict\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid vector database!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Git Projects/RAG-Optimization/vector_store.py:61\u001b[39m, in \u001b[36mQdrantVectorStore.__init__\u001b[39m\u001b[34m(self, knowledge_base, config_dict)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# TODO: add reranking\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# TODO: experiment with filtering methods\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# TODO: add function to update the vector db with additional documents\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.knowledge_base = knowledge_base\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_config_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Git Projects/RAG-Optimization/vector_store.py:91\u001b[39m, in \u001b[36mQdrantVectorStore.set_config_options\u001b[39m\u001b[34m(self, config_dict)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embeddings_platform == \u001b[33m\"\u001b[39m\u001b[33mOpenAI\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mself\u001b[39m.embeddings_client = OpenAI()\n\u001b[32m     90\u001b[39m test_vector = (\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings_client\u001b[49m.embeddings.create(\n\u001b[32m     92\u001b[39m         \u001b[38;5;28minput\u001b[39m=[\u001b[33m\"\u001b[39m\u001b[33mtest embeddings\u001b[39m\u001b[33m\"\u001b[39m], model=\u001b[38;5;28mself\u001b[39m.embeddings_model_name\n\u001b[32m     93\u001b[39m     )\n\u001b[32m     94\u001b[39m     .data[\u001b[32m0\u001b[39m]\n\u001b[32m     95\u001b[39m     .embedding\n\u001b[32m     96\u001b[39m )\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.vector_size = \u001b[38;5;28mlen\u001b[39m(test_vector)\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.vector_distance = Distance.COSINE\n",
      "\u001b[31mAttributeError\u001b[39m: 'QdrantVectorStore' object has no attribute 'embeddings_client'"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    #\"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"qdrant\",\n",
    "    \"sparse_text_model\": \"Qdrant/bm25\",\n",
    "    \"models\": {\n",
    "       # \"meta/meta-llama-3-70b-instruct\":\n",
    "       \"anthropic/claude-3.5-sonnet\": \"Replicate\"},\n",
    "    \"embed_options\": {\n",
    "        \"text-embedding-3-small\": \"OpenAI\"\n",
    "        }, \n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "vector_db_folder = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "optimize_rag_parameters(\n",
    "    df_to_test, \n",
    "    langchain_docs, \n",
    "    parameters_dict,\n",
    "    results_folder=results_folder, \n",
    "    vector_db_folder=vector_db_folder\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
