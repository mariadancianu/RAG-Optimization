{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0accd0d-fb0b-4c5e-be02-8063d3b6a2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from rag_fine_tuning import convert_knowledge_base_to_langchain_docs, fine_tune_rag\n",
    "from data_utils import convert_json_to_dataframe, create_json_subset, collect_all_results, merge_results\n",
    "sns.set_style(\"whitegrid\")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece24fd",
   "metadata": {},
   "source": [
    "# Useful material \n",
    "# SQuAD Evaluation guidelines: \n",
    "# https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/\n",
    "# https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b749b",
   "metadata": {},
   "source": [
    "# Convert json data to pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d8df1-cea0-410b-9c27-985406ea208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_json_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = pd.read_csv(\"dataset.csv\")\n",
    "df_all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771778e-ae9e-49b4-9216-658bdefc8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2eebd-70b9-4118-b691-0c6c96e72a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some rough statistics for the context length \n",
    "df_selected.loc[:, \"context_chars\"] = df_selected[\"context\"].apply(lambda x: len(x))\n",
    "df_selected.loc[:, \"context_words\"] = df_selected.loc[:, \"context\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f66f8e-3eb9-4943-9b2e-40df68068a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cab993-b3ab-4947-acb5-016382d1321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "sns.histplot(df_selected.drop_duplicates(subset=\"context\")[\"context_chars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb1095-e0d8-4f06-b203-08ff1689aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8, 5])\n",
    "sns.histplot(df_selected.drop_duplicates(subset=\"context\")[\"context_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9848fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create original json structure for only a subset of questions, used for tests and fine-tuning \n",
    "# this file will be used by the evaluation.py file \n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_sel = df[0:500]\n",
    "df_sel.head(2)\n",
    "\n",
    "create_json_subset(df_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c8057-750e-44d6-81c8-d8ad6717b311",
   "metadata": {},
   "source": [
    "# RAG architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf8664-3381-4701-b297-bbcfa5176872",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "**Data Indexing**\n",
    "\n",
    "Converting text data into a searchable database of vector embeddings, which represent the meaning of the text in a format that computers can easily understand.\n",
    "- **Documents Chunking**: The collection of documents is split into smaller chunks of text. This allows for more precise and relevant pieces of information to be fed into the language model when needed, avoiding information overload.\n",
    "- **Vector Embeddings**: The chunks of text are then transformed into vector embeddings. These embeddings encode the meaning of natural language text into numerical representations.\n",
    "- **Vector Database**: Finally, the vector embeddings are stored in a vector database, making them easily searchable.\n",
    "\n",
    "**Documents -> Text chunks -> Vector Embeddings -> Vector DB**\n",
    "\n",
    "**Load -> Split -> Embed -> Store**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfc599",
   "metadata": {},
   "source": [
    "## Convert the pandas context to Langchain documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957f241f-d54c-452e-94ad-cfcc8a81a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d8e92a-9322-4359-b18b-f4217fb78003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1204\n",
      "page_content='The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.' metadata={'title': 'Normans'}\n",
      "page_content='The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic orthodoxy into which they assimilated. They adopted the Gallo-Romance language of the Frankish land they settled, their dialect becoming known as Norman, Normaund or Norman French, an important literary language. The Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in feudal tenure. The Normans are noted both for their culture, such as their unique Romanesque architecture and musical traditions, and for their significant military accomplishments and innovations. Norman adventurers founded the Kingdom of Sicily under Roger II after conquering southern Italy on the Saracens and Byzantines, and an expedition on behalf of their duke, William the Conqueror, led to the Norman conquest of England at the Battle of Hastings in 1066. Norman cultural and military influence spread from these new European centres to the Crusader states of the Near East, where their prince Bohemond I founded the Principality of Antioch in the Levant, to Scotland and Wales in Great Britain, to Ireland, and to the coasts of north Africa and the Canary Islands.' metadata={'title': 'Normans'}\n"
     ]
    }
   ],
   "source": [
    "print(len(langchain_docs))\n",
    "print(langchain_docs[0])\n",
    "print(langchain_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2075d54",
   "metadata": {},
   "source": [
    "## Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de25a07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"CustomRAG config: {'chunk_size': 400, 'chunk_overlap': 15, \"\n",
      " \"'vector_database': 'chromadb', 'embeddings_function': {'model_name': \"\n",
      " \"'text-embedding-3-large', 'platform': 'OpenAI'}, 'llm': {'model_name': \"\n",
      " \"'gpt-3.5-turbo', 'client': 'OpenAI'}}\")\n"
     ]
    }
   ],
   "source": [
    "from rag_fine_tuning import CustomRAG, prompt_message, convert_knowledge_base_to_langchain_docs\n",
    "\n",
    "parameters_dict = {\n",
    "    \"chunk_size\": 400,\n",
    "    \"chunk_overlap\": 15,\n",
    "    \"vector_database\": \"chromadb\",\n",
    "    \"embeddings_function\": {\n",
    "        \"model_name\": \"text-embedding-3-large\",    \n",
    "        \"platform\": \"OpenAI\"\n",
    "        }, \n",
    "    \"llm\": {\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"client\": \"OpenAI\"\n",
    "        }\n",
    "}\n",
    "\n",
    "df_to_test = df[0:2]\n",
    "\n",
    "rag = CustomRAG(knowledge_base=langchain_docs, \n",
    "                prompt_message=prompt_message,\n",
    "                config=parameters_dict, \n",
    "                results_folder='/Users/mariadancianu/Desktop/Git Projects/SQuAD_RAG_experiments/eval_results/test_new_class', \n",
    "                vector_db_folder='/Users/mariadancianu/Desktop/Git Projects/SQuAD_RAG_experiments/vector_databases/test_new_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d08f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.initialize_embeddings_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9878b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store 400_text-embedding-3-large already exists. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = \"text-embedding-3-small\"\n",
    "embeddings = OpenAIEmbeddings(model=embeddings_model)\n",
    "\n",
    "db_dir = os.path.join(os.getcwd(), \"vector_databases\")\n",
    "\n",
    "rag.create_vector_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0571139",
   "metadata": {},
   "source": [
    "## Querying the vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c5fcd1-6677-4a48-823c-7b899bfee9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying chroma vector store\n",
      "How is the weather today in Milan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariadancianu/Desktop/Git Projects/SQuAD_RAG_experiments/rag_fine_tuning.py:263: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n",
      "No relevant docs were retrieved using the relevance score threshold 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "query = \"How is the weather today in Milan?\"\n",
    "relevant_docs = rag.query_vector_store(query, n_results=3, score_threshold=0.1)\n",
    "\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d473dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying chroma vector store\n",
      "Who were the normans?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "query = \"Who were the normans?\"\n",
    "relevant_docs = rag.query_vector_store(query, n_results=3, score_threshold=0.1)\n",
    "\n",
    "print(len(relevant_docs))\n",
    "\n",
    "for doc in relevant_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5aa673",
   "metadata": {},
   "source": [
    "## Run the RAG over a subset of questions and save the answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb0a1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_test = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df_to_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff098d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store 400_text-embedding-3-large already exists. No need to initialize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying chroma vector store\n",
      "In what country is Normandy located?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Context: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:02<00:08,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying chroma vector store\n",
      "When were the Normans in Normandy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Context: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:03<00:04,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying chroma vector store\n",
      "From which countries did the Norse originate?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Context: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:05<00:04,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying chroma vector store\n",
      "Who was the Norse leader?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Context: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:07<00:01,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying chroma vector store\n",
      "What century did the Normans first gain their separate identity?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Context: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:08<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rag.get_llm_multiple_questions_answers(df_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11654ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b204b4fa",
   "metadata": {},
   "source": [
    "## RAG Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273535f0",
   "metadata": {},
   "source": [
    "Comments\n",
    "Peak distribution for number of characters in the documents is ~600 words. \n",
    "- smaller chunks: reduced noise from irrelevant content - works well with dense embeddings*\n",
    "- larger chunks: preserves context better; ideal if queries require full document context, works well with hybrid search*\n",
    "\n",
    "TEXT CHUNKING \n",
    "\n",
    "1. CHARACTER SPLITTING : divide the text into N-character sized chunks. Can split words in the middle. \n",
    "2. RECURSIVE CHARACTER SPLITTING: preserves sentences. Avoids splitting sentences midword (note that RecursiveCharacterTextSplitter with separator does exactly that). Split the\n",
    "document where a double new line is present, then, if the chunk size is still exceeded, split at new lines, and so on.\n",
    "3. SEMANTIC SPLITTING: keeps related content together. Use embeddings to split based on meaning.\n",
    "+ other techniques\n",
    "\n",
    "EMBEDDINGS \n",
    "Create fixed-length vector representation of text, focusing on semanting meaning for tasks like similarity comparison. \n",
    "Most up to date embedding models, both proprietary and open source, with performance metrics across different tasks: https://huggingface.co/spaces/mteb/leaderboard \n",
    "This contains also a \"retrieval\" column with performance metrics. Click on the column to sort the models.\n",
    "Interesting article: https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0830832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "fine_tune_rag(df_to_test, \n",
    "              langchain_docs, \n",
    "              results_fodler=\"eval_results/test_new_class\", \n",
    "              vector_db_folder=\"vector_databases/test_new_class\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_res = collect_all_results('/Users/mariadancianu/Desktop/Git Projects/SQuAD_RAG_experiments/eval_results/initial_eval_results')\n",
    "df_all_res.sort_values(by=\"HasAns_f1\", ascending=False, inplace=True)\n",
    "df_all_res.to_csv(\"eval_results/initial_eval_results/df_all_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835179f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c923406",
   "metadata": {},
   "source": [
    "# Investigate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008528cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best results and merge the scores by question id to the original df in order to inspect the errors.\n",
    "# The idea is to understand why the results are so poor for the NoAns questions, when the HasAns questions have \n",
    "# a high f1 score, in order to understand how the workflow can be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeaa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_path = os.path.join(os.getcwd(), \"eval_results/initial_eval_results\", df_all_res.experiment.iloc[0])\n",
    "split_path = best_result_path.split(\"/\")\n",
    "split_path[-1] = split_path[-1].replace(\"eval_\", \"\")\n",
    "best_result_path = \"/\".join(split_path)\n",
    "best_result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with best parameters, and save also the context\n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "        \"text-embedding-3-large\": \"OpenAI\", \n",
    "        },\n",
    "    \"models\": {\"gpt-3.5-turbo\": \"openai\"}\n",
    "}\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/debugging_eval_results\")\n",
    "results_folder\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "fine_tune_rag(df_to_test, langchain_docs, parameters_dict, results_folder=results_folder, save_context=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979903fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1beec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python eval_results/evaluation.py \"/Users/mariadancianu/Desktop/Git Projects/SQuAD_RAG_experiments/eval_results/data_updated_500.json\" \"/Users/mariadancianu/Desktop/Git Projects/SQuAD_RAG_experiments/eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_results(f1_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/f1_thresh_by_qid.json\"), \n",
    "                          exact_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/exact_thresh_by_qid.json\"), \n",
    "                          pred_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"), \n",
    "                          filepath_500=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/pred_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"),\n",
    "                          context_filepath=os.path.join(os.getcwd(), \"eval_results/debugging_eval_results/context_500_400_text-embedding-3-large_gpt-3.5-turbo.json\"), \n",
    "                          df_questions_filepath=\"dataset.csv\", \n",
    "                          filter_500=True)\n",
    "\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585095c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9391a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.is_impossible][[\"id\", \"is_impossible\", \"f1_score\", \"exact_score\", \"question\", \"pred\"]].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.loc[479, \"context\"].replace(\". \", \".\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.loc[479, \"rag_retrieved_context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cef210",
   "metadata": {},
   "source": [
    "# Evaluate with other LLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with best parameters, and save also the context\n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "        \"text-embedding-3-large\": \"OpenAI\", \n",
    "        },\n",
    "    \"models\": {\"mistral-large-latest\": \"mistral\"}\n",
    "}\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "results_folder\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "fine_tune_rag(df_to_test, langchain_docs, parameters_dict, results_folder=results_folder, save_context=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765bcaf5",
   "metadata": {},
   "source": [
    "# Evaluate RAG SOTA embeddings: snowflake-artic-embed-l-v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2faae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the RAG with SOTA embeddings \n",
    "parameters_dict = {\n",
    "    \"chunk_sizes\": [400],\n",
    "    \"embed_options\": { \n",
    "        \"Snowflake/snowflake-arctic-embed-l-v2.0\": \"HuggingFace_SentenceTransformers\" # ranked 6th, 568M params, released in december 2024 \n",
    "        },\n",
    "    \"models\": {\"gpt-3.5-turbo\": \"openai\"}\n",
    "}\n",
    "\n",
    "results_folder = os.path.join(os.getcwd(), \"eval_results/optimize_results\")\n",
    "results_folder\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_to_test = df[0:500]\n",
    "\n",
    "langchain_docs = convert_knowledge_base_to_langchain_docs(df)\n",
    "\n",
    "fine_tune_rag(df_to_test, langchain_docs, parameters_dict, results_folder=results_folder, save_context=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009a96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a86af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca88e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
